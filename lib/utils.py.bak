# utils.py
import aiohttp
import asyncio
import zipfile
from datetime import datetime
from pathlib import Path
from typing import List, Optional
import subprocess
import chardet
from rich.progress import Progress, TaskID, TextColumn, BarColumn, DownloadColumn, TransferSpeedColumn, TimeRemainingColumn
from rich.console import Console
import nest_asyncio

nest_asyncio.apply()

# Progress Setup
def get_console() -> Console:
    return Console()

def create_progress(**kwargs) -> Progress:
    return Progress(
        TextColumn("[bold blue]{task.description}[/bold blue]"),
        BarColumn(),
        "[progress.percentage]{task.percentage:>3.0f}%",
        "•",
        DownloadColumn(),
        "•",
        TransferSpeedColumn(),
        "•",
        TimeRemainingColumn(),
        console=get_console(),
        expand=True,
        **kwargs
    )

def get_zip_urls(year_month: str, name:str) -> list[str]:
    """Generate URLs for CNPJ zip files for a given year-month."""
    base_url = f"https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/{year_month}/"
    return [f"{base_url}{name}{i}.zip" for i in range(10)]


# Download Functions
async def download_file(session, url, progress, task_id, path, max_retries=5, initial_delay=2):
    for attempt in range(max_retries):
        temp_path = path.with_suffix('.tmp')  # Use temp file during download
        try:
            # Remove any existing temp files
            if temp_path.exists():
                temp_path.unlink()

            async with session.get(url) as response:
                response.raise_for_status()
                total = int(response.headers.get('content-length', 0))
                
                # Handle unknown file sizes
                progress.update(
                    task_id, 
                    total=total if total > 0 else None,
                    description=f"Downloading {path.name} (Attempt {attempt+1})"
                )

                downloaded = 0
                with open(temp_path, 'wb') as f:
                    async for chunk in response.content.iter_chunked(2 * 1024 * 1024):  # 2MB chunks
                        f.write(chunk)
                        downloaded += len(chunk)
                        progress.update(task_id, completed=downloaded)

                # Validate size if content-length was provided
                if total > 0 and temp_path.stat().st_size != total:
                    raise aiohttp.ClientPayloadError(
                        f"Size mismatch: Expected {total}, got {temp_path.stat().st_size}"
                    )

                # Atomic rename when complete
                temp_path.rename(path)
                return True

        except (aiohttp.ClientPayloadError, aiohttp.ServerDisconnectedError, 
                ConnectionResetError, aiohttp.ClientOSError) as e:
            # Cleanup failed attempts
            if temp_path.exists():
                temp_path.unlink()
            if attempt < max_retries - 1:
                delay = initial_delay * (2 ** attempt) + random.uniform(0, 1)
                progress.console.log(f"Retrying {path.name} in {delay:.1f}s: {str(e)}")
                await asyncio.sleep(delay)
                continue
            raise
        finally:
            if temp_path.exists():
                temp_path.unlink()

# Extraction Functions
async def unzip_worker_sync(zip_path: Path, extract_path: Path, progress: Progress):
    def sync_unzip():
        # Use low-level ZIP handling for better memory management
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            for file_info in zip_ref.infolist():
                if file_info.file_size > 1 * 1024**3:  # 1GB threshold
                    # Extract large files in chunks
                    with zip_ref.open(file_info) as source, \
                         open(extract_path / file_info.filename, 'wb') as target:
                        while chunk := source.read(4 * 1024**2):  # 4MB chunks
                            target.write(chunk)
                else:
                    zip_ref.extract(file_info, extract_path)
# async def unzip_worker_sync(zip_path: Path, extract_path: Path, progress: Progress) -> List[str]: # Return type hint added
#     task_id = progress.add_task(f"Extracting {zip_path.name}", total=100)
#     extracted_files: List[str] = [] 

#     try:
#         def sync_unzip():
#             nonlocal extracted_files  # To modify the extracted_files in the outer scope
#             with zipfile.ZipFile(zip_path, 'r') as zip_ref:
#                 extracted_files = zip_ref.namelist()
#                 zip_ref.extractall(extract_path)

#         await asyncio.to_thread(sync_unzip)
#         progress.update(task_id, completed=100)

#     except Exception as e:
#         get_console().print(f"[red]Extraction failed for {zip_path}: {str(e)}[/red]")
#         raise
#     return extracted_files 

# Conversion Functions
async def convert_worker_sync(file_path: Path, progress: Progress) -> None:
    # Use streaming conversion instead of loading entire file
    try:
        with open(file_path, 'rb') as src:
            with open(temp_file, 'w', encoding='utf-8') as dst:
                while chunk := src.read(4 * 1024**2):  # 4MB chunks
                    decoded = chunk.decode('iso-8859-1').encode('utf-8')
                    dst.write(decoded.decode('utf-8'))
    except Exception as e:
        # Handle partial cleanup
        if temp_file.exists():
            temp_file.unlink()
        raise
# async def convert_worker_sync(file_path: Path, progress: Progress) -> None:
#     task_id = progress.add_task(f"Converting {file_path.name}", total=100)
#     temp_file = file_path.with_suffix('.tmp')
#     backup_file = file_path.with_suffix('.bak')
    
#     try:
#         file_path.rename(backup_file)
#         result = subprocess.run(
#             [
#                 'iconv', 
#                 '-f', 'ISO-8859-1', 
#                 '-t', 'UTF-8//TRANSLIT',
#                 str(backup_file)
#             ],
#             capture_output=True,
#             text=True,
#             check=True
#         )
#         with open(temp_file, 'w', encoding='utf-8') as f:
#             f.write(result.stdout)
        
#         # Atomic replacement
#         temp_file.replace(file_path)
#         backup_file.unlink()
        
#         # Verify conversion
#         verify_result = subprocess.run(
#             ['file', '-I', str(file_path)],
#             capture_output=True,
#             text=True
#         )
#         encoding = verify_result.stdout.lower()
#         if 'utf-8' not in encoding and 'us-ascii' not in encoding: # us-ascii is a subset of utf-8
#             raise ValueError(f"Conversion verification failed: {verify_result.stdout}")
#         progress.update(task_id, completed=100)
        
#     except Exception as e:
#         # Cleanup and restore on failure
#         if temp_file.exists():
#             temp_file.unlink()
#         if backup_file.exists():
#             backup_file.replace(file_path)
#         get_console().print(f"[red]Conversion failed for {file_path}: {str(e)}[/red]")
#         raise

import random

# List of realistic user agents
USER_AGENTS = [
    # Firefox on Mac
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/114.0",
    # Safari on Mac
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15",
    # Edge on Windows
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36 Edg/114.0.1823.43"
]

def get_random_user_agent() -> str:
    """Get a random realistic user agent"""
    return random.choice(USER_AGENTS)

async def download_unzip_convert(
    download_path: Path,
    csvname: str,
    zip_urls: List[str],
    workers: int = 8
) -> None:
    download_path.mkdir(parents=True, exist_ok=True)
    
    # Create session with random user agent and throttling
    headers = {
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.5',
        'Connection': 'keep-alive'
    }
    
    # Add rate limiting
    timeout = aiohttp.ClientTimeout(
        total=30 * 60,  # 30 minutes
        sock_connect=30, 
        sock_read=300
    )
    
    # Reduce concurrent connections to avoid overwhelming server
    connector = aiohttp.TCPConnector(
        limit_per_host=2,  # Reduced from 8
        enable_cleanup_closed=True,  # Better handle closed connections
        ssl=False
    )
    
    async with aiohttp.ClientSession(
        headers=headers,
        connector=connector,
        timeout=timeout
    ) as session:
        with create_progress() as progress:
            tasks = []
            for url in zip_urls:
                zip_file = download_path / url.split('/')[-1]
                tasks.append(process_single_zip(session, progress, url, zip_file, download_path))
                
            # Add random delays between requests
            for task in asyncio.as_completed(tasks):
                await task
                await asyncio.sleep(random.uniform(0.5, 1.5))  # Random delay between 0.5-1.5 seconds

async def process_single_zip(
    session: aiohttp.ClientSession,
    progress: Progress,
    url: str,
    zip_file: Path,
    download_path: Path
) -> None:
    """Process a single ZIP file with async pipeline"""
    try:
        # 1. Download
        download_task = progress.add_task(f"Downloading {zip_file.name}", total=100)
        await download_file(session, url, progress, download_task, zip_file)
        
        # 2. Unzip
        extracted_files = await unzip_worker_sync(zip_file, download_path, progress)
        
        # 3. Convert all extracted files concurrently
        convert_tasks = []
        for extracted in extracted_files:
            convert_tasks.append(convert_worker_sync(download_path / extracted, progress))
        await asyncio.gather(*convert_tasks)
        
        
        zip_file.unlink()
        
    except Exception as e:
        get_console().print(f"[red]Failed processing {zip_file.name}: {str(e)}[/red]")
        raise

def write_completion_marker(download_path: Path, csvname: str, zip_urls: List[str]) -> None:
    (download_path / f"{csvname}.md").write_text(
        f"Completed: {datetime.now().isoformat()}\n"
        f"Processed {len(zip_urls)} files\n"
        f"Encoding conversion: utf-8"
    )
